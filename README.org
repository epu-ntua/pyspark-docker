
This repo helps you to run Spark (on Docker) with the following installed:

1. pySpark on Hadoop
2. numpy
3. scikit-learn

* Setting up (one-time only)

** On Mac:

1. Install [[http://brew.sh][homebrew]]
2. Install [[https://www.virtualbox.org/wiki/Downloads][Virtualbox]]
3. Install boot2docker:
#+begin_src sh
brew install boot2docker
boot2docker init
boot2docker up
# The following URL is output as a result of the above "boot2docker up" command. 
export DOCKER_HOST=tcp://192.168.59.103:2375
#+end_src

4. Install docker:
#+begin_src sh
brew install docker
docker version
#+end_src

** On other OSes

Make sure that docker is installed and is runnable from the command-line.  

* Starting pyspark

1. Build the docker image, by running the following command in the
   same directory as this =README= file. (The command will be slow the
   first time (since numpy and scikitlearn need to be compiled from
   source, but the result is cached. This step should only be repeated
   if you modify =Dockerfile=)

#+begin_src sh
$ docker build -t pyspark-docker .
#+end_src

2. Run the following command to start the container and get a prompt

#+begin_src sh
docker run -i -t -h pyspark-docker /etc/bootstrap.sh -bash
#+end_src

3. Start pyspark:

#+begin_src 
cd /usr/local/spark/bin
./pyspark
#+end_src

That's it! :)

